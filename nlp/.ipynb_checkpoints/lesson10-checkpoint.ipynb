{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lecture-10 Build a neural network from scratch\n",
    "\n",
    "## 框架\n",
    "什么是框架：已经定好的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓扑排序在神经网络中的应用： 反向传播算法计算梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node:\n",
    "- forward: function, how to calculate the inputs\n",
    "- backwards: function, how to get gradients when backpropagation\n",
    "- input: list, the input nodes of this node\n",
    "- output: list, the output nodes of this node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        if the node is operator of ax+b , the inputs will be\n",
    "        x node, and the outputs is its successor\n",
    "        value :  ax+b\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.value = None\n",
    "        self.gradients = {}\n",
    "    \n",
    "        for node in self.inputs:\n",
    "            node.outputs.append(self)\n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    def backward(self):\n",
    "        raise NotImplemented\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__(self,inputs={},outputs={})\n",
    "    \n",
    "    def forward(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "    def backward(self):\n",
    "        self.gradients = {}\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost # todo \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear(Node):\n",
    "    def __init__(self,nodes,weights,bias):\n",
    "        self.w_node = weights\n",
    "        self.x_node = nodes\n",
    "        self.b_node = bias\n",
    "        Node.__init__(self, inputs=(nodes, weight, bias))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = np.dot(self.x_node.value, self.w_node.value)+ self.b_node.value\n",
    "    \n",
    "    def backward(self):\n",
    "        for node in self.outputs:\n",
    "            grad_cost = node.gradient(self)\n",
    "            \n",
    "            self.gradients[self.w_node] = np.dot(self.x_node.value.T,grad_cost)\n",
    "            self.gradients[self.b_node] = np.sum(grad_cost,axis = 0, keepdim=False)\n",
    "            self.gradients[self.x_node] = np.dot(grad_cost, self.w_node.value.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(Node):\n",
    "    def __init__(self, Node):\n",
    "        Node.__init__(self, [node])\n",
    "        self.x_node = node\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1+np.exp(-1*x))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.value = self._sigmoid(self.x_node.value)\n",
    "    \n",
    "    def backward(self):\n",
    "        y = self.value\n",
    "        \n",
    "        self.partial = y*(1-y)\n",
    "        \n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self.x_node] = grad_cost*self.partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y_true, y_hat):\n",
    "        self.y_true_node = y_true\n",
    "        self.y_hat_node = y_hat\n",
    "        Node.__init__(self, inputs=[y_true, y_hat])\n",
    "    \n",
    "    def forward(self):\n",
    "        y_true_flatten = self.y_true_node.value.reshape(-1, 1)\n",
    "        y_hat_flatten = self.y_hat_node.value.reshape(-1,1)\n",
    "        \n",
    "        self.diff = y_true_flatten -y_hat_flatten\n",
    "        self.value = np.mean((y_true_flatten-y_hat_flatten)**2)\n",
    "        \n",
    "    def backward(self):\n",
    "        n = self.y_hat_node.shape[0]\n",
    "        \n",
    "        self.gradients[self.y_true_node] = (2/n)*self.diff\n",
    "        self.gradients[self.y_hat_node] = (-2/n)*self.diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_one_epoch(graph):\n",
    "    # graph 是经过拓扑排序之后的list\n",
    "    for node in graph:\n",
    "        node.forward()\n",
    "    \n",
    "    for node in graph[::-1]:\n",
    "        node.backword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(trainable_nodes, learning_rate=1e-2):\n",
    "    for t in trainable_nodes:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
