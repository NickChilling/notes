{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 11\n",
    "## Word2Vec Models\n",
    "\n",
    "原理: 如果$A$ 和 $A^*$ 周围的单词相似, 那么他们的词向量表征也应该相似(距离相近)\n",
    "\n",
    "神经网络的最后一层输出 \n",
    "\n",
    "对于每一个单词,随机初始一个向量.\n",
    "\n",
    "\n",
    "向量经过一层神经网络的projection, 产生一个$vocabulary_size*1$ 的输出.经过softmax之后, \n",
    "\n",
    "\n",
    "该向量中每个scalar对应词库中该单词是否与输入单词相邻.\n",
    "\n",
    "\n",
    "CBOW \n",
    "\n",
    "输入周围单词, 输出词向量\n",
    "Skip-Ngram\n",
    "输入单个词向量, 输出周围单词\n",
    "<img src=\"https://raw.githubusercontent.com/NickChilling/notes/master/pic_bed/20190928111655.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样就带来了一个问题: 如果词库过大,如几万上百万.每训练一个单词就需要更新`softmax`那一层(voc_size, 1)大小的权值. \n",
    "\n",
    "这样计算的开销就比较大\n",
    "\n",
    "## 改进\n",
    "### hierarchical softmax\n",
    "因此选择将最后一层, 改为一个二叉树. -> hierachical softmax \n",
    "\n",
    "这样每次只需要沿着路径更新$\\log(n)$ 个节点? \n",
    "\n",
    "- [ ] 具体实现方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling\n",
    "\n",
    "修改了损失函数。在普通的损失函数中，计算了词库中所有词的交叉熵。在反向传播时，需要对最后一层Voc_size数量的神经元回馈。\n",
    "\n",
    "Negative Sampling 只需要考虑10多个神经元。 \n",
    "\n",
    "$$J(\\theta)=\\log{\\sigma({u_0}^TV_c)}+ \\sum{\\log{\\sigma(-{u_j}^TV_c}}$$\n",
    "\n",
    "\n",
    "\n",
    "对于每个词$V_c$, 希望它附近的词$u_0$和它的方向相似。不在同一个context下的单词$u_j$ 方向不同。\n",
    "\n",
    "而$u_j$不是直接获取，而是依据该词的概率采样。\n",
    "\n",
    "要采多少个？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fast text\n",
    "\n",
    "针对 skip-gram 中存在的问题进行进一步的改进。 \n",
    "\n",
    "问题： skip-gram无法识别更细粒度词的关系。 如： 阿里巴巴和蚂蚁金服的关系， 模型可以识别。 但无法识别 阿里 和 蚂蚁 的关系。 \n",
    "\n",
    "那么就对词作进一步的拆分。 给定超参数n。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['阿里', '里巴', '巴巴', '巴和', '和蚂', '蚂蚁', '蚁金']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fast_text_sample(s=\"阿里巴巴和蚂蚁金服\", n = 2):\n",
    "    grams = []\n",
    "    for i in range(len(s)-n):\n",
    "        grams.append(s[i:i+n])\n",
    "    return grams\n",
    "fast_text_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以拆分结果作为特征,带入到模型中训练.\n",
    "如何训练?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glove\n",
    "\n",
    "预处理-> co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
