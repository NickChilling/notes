# a simple but tough-to-beat baseline for sentence embedding

## 摘要

本文提出了一个基于词向量生成句向量的无监督学习方法。简而言之，利用`Word2Vec`方法得到词向量。将句内的词向量以乘以各自的权重相加。再将相加得到的加权平均向量PAC降维得到句向量。基于神经网络产生长文本向量的方法很多，效果也非常好。但问题在于需要大量有标签数据进行监督学习。本文提出的方法不需要标签，效果也超过了神经网络。

## 算法

对于一个句子s，包括t个词/字$\{W_1,W_2\dots W_t\}$。对每个词$i$都能通过`Word2Vec`计算出一个词向量$W_i$，它的权重为$\frac a {a+p(W_i)}$。其中a是超参数，而$P(W_i)$是该词在文章中的词频。将句中各词按照频率加权得到向量$C$,$C=\frac 1 {|s|} \sum\limits_i^t W_i*{\frac a {a+P(W_i)}}$
称为平滑逆频率(smooth inverse frequency SIF)

### PCA 降维

PCA降维实际上是通过改变表征向量的基进行降维。
对于m个n维样本。我们有特征矩阵 X(m,n)。
通过计算n个维度上协方差。得到协方差矩阵。Cov(n,n)
计算Cov的n个特征值和特征向量。由大到小排序，我们选择前a个特征值和特征向量。那么用a个特征向量可以拼接为一个矩阵Eigen(n,a)
将X与a进行矩阵相乘 X(m,n)*Eigen(n,a) = Mat(m,a)这样就把特征维度降到了a维。
那么如何计算解释程度？ 在这个过程中损失了多少信息？